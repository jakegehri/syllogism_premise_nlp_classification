{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "058d4784-8fec-4258-aba9-274c8c5ad74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_metric\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5bbad9b3-321e-4b7d-9414-830b33a16aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Avicenna_Train.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7c8b8de3-6437-4d05-852a-2ff260dc8215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Premise 1</th>\n",
       "      <th>Premise 2</th>\n",
       "      <th>Syllogistic relation</th>\n",
       "      <th>Conclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unchecked imbalances in the society, will see ...</td>\n",
       "      <td>correct these imbalances requires in-depth kno...</td>\n",
       "      <td>no</td>\n",
       "      <td>No conclusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chronic diseases are heart attacks and stroke,...</td>\n",
       "      <td>In populations that eat a regular high-fiber d...</td>\n",
       "      <td>yes</td>\n",
       "      <td>In populations that eat a regular high-fiber d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Formative assessment encourages children to en...</td>\n",
       "      <td>An ideal learning environment uses formative a...</td>\n",
       "      <td>yes</td>\n",
       "      <td>An ideal learning environment encourages child...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Underrepresented female labor force in some pr...</td>\n",
       "      <td>Job discrimination comes with underrepresented...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Job discrimination comes with not being able t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>damaged mentality in an individual brings seri...</td>\n",
       "      <td>Aggression harms the mentality of person.</td>\n",
       "      <td>yes</td>\n",
       "      <td>Aggression brings brings serious health proble...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Premise 1  \\\n",
       "0  unchecked imbalances in the society, will see ...   \n",
       "1  Chronic diseases are heart attacks and stroke,...   \n",
       "2  Formative assessment encourages children to en...   \n",
       "3  Underrepresented female labor force in some pr...   \n",
       "4  damaged mentality in an individual brings seri...   \n",
       "\n",
       "                                           Premise 2 Syllogistic relation  \\\n",
       "0  correct these imbalances requires in-depth kno...                   no   \n",
       "1  In populations that eat a regular high-fiber d...                  yes   \n",
       "2  An ideal learning environment uses formative a...                  yes   \n",
       "3  Job discrimination comes with underrepresented...                  yes   \n",
       "4          Aggression harms the mentality of person.                  yes   \n",
       "\n",
       "                                          Conclusion  \n",
       "0                                      No conclusion  \n",
       "1  In populations that eat a regular high-fiber d...  \n",
       "2  An ideal learning environment encourages child...  \n",
       "3  Job discrimination comes with not being able t...  \n",
       "4  Aggression brings brings serious health proble...  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0e65a77f-7b1d-40d7-baeb-ee1555c4b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['Syllogistic relation'].eq('yes').mul(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9bb44e13-f30f-4c99-8242-e1715ebc4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = (df['Premise 1'] + \" : \" + df['Premise 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6777f917-7399-488c-8367-0c40d178fe6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3840"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(df) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0e84f01d-abb6-460b-9671-8e93b52f94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df.iloc[:3840]['text'].values\n",
    "train_labels = df.iloc[:3840]['label'].values\n",
    "\n",
    "valid_texts = df.iloc[3840:]['text'].values\n",
    "valid_labels = df.iloc[3840:]['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8f424bda-7bbd-4171-8c98-585748d4d07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e84f262d-d672-4ecd-ba36-78301d3feb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(list(valid_texts), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d67694cd-9902-49de-900a-8327cf6e1619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyllogismDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ae8094b4-e736-4a8c-9130-6eb1a96d4b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SyllogismDataset(train_encodings, train_labels)\n",
    "valid_dataset = SyllogismDataset(valid_encodings, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "97074b20-53f2-4090-9b7a-c49f6c907149",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader2(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader2(valid_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "795ec79a-d239-446b-8d40-e5c97bcf854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2a2c2dde-0051-4402-bd9b-843729cbcaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "87351fcf-38d2-40ba-9e67-50a27584e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "metrics = load_metric('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6e15ba77-4aaa-41b2-aca2-3463a8e41af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metrics.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1721d5f2-b33d-4ef9-b168-202b8618d745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir='./results', num_train_epochs=3, per_device_train_batch_size=16,\n",
    "                                 per_device_eval_batch_size=16, logging_dir='./logs', logging_steps=72)\n",
    "\n",
    "trainer = Trainer(model=model, \n",
    "                  args=training_args, \n",
    "                  train_dataset=train_dataset, \n",
    "                  eval_dataset=valid_dataset,\n",
    "                  compute_metrics=compute_metrics, \n",
    "                  optimizers=(optim, None)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "90addaf4-e95c-45de-a2f2-832193eb8b94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3840\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 720\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [720/720 01:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.651100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.503400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>0.246500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>0.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.105100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=720, training_loss=0.30067731738090514, metrics={'train_runtime': 111.7538, 'train_samples_per_second': 103.084, 'train_steps_per_second': 6.443, 'total_flos': 289110097566720.0, 'train_loss': 0.30067731738090514, 'epoch': 3.0})"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cd07ac23-ae77-430a-bdfc-78e9addee147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 960\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='235' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 22:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.42222920060157776,\n",
       " 'eval_accuracy': 0.878125,\n",
       " 'eval_runtime': 2.3711,\n",
       " 'eval_samples_per_second': 404.88,\n",
       " 'eval_steps_per_second': 25.305,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9e284052-20e3-4aef-bd90-ab5bde022e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('Avicenna_Test.csv', encoding='ISO-8859-1')\n",
    "\n",
    "df_test['label'] = df_test['Syllogistic relation'].eq('yes').mul(1)\n",
    "df_test['text'] = (df_test['Premise 1'] + \" : \" + df_test['Premise 2'])\n",
    "\n",
    "test_texts = df_test['text'].values\n",
    "test_labels = df_test['label'].values\n",
    "\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\n",
    "\n",
    "test_dataset = SyllogismDataset(test_encodings, test_labels)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ee05dc74-b8a2-4e9f-ad91-6be5028519d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1200\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5125078558921814,\n",
       " 'eval_accuracy': 0.86,\n",
       " 'eval_runtime': 4.5547,\n",
       " 'eval_samples_per_second': 263.466,\n",
       " 'eval_steps_per_second': 16.467,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "1620318c-051f-4511-9fbd-7f6ba4cdfe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = ['Socrates is a man : all men are mortal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "720d2bc6-efce-4654-9e84-9a8c063777c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoded = tokenizer(sample_text, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e81f36d0-98f2-4633-8b89-9febd61e2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = SyllogismDataset(sample_encoded, sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "6a93aacc-f065-4ded-97cb-6fe2357daaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(sample_encoded['input_ids']).to(DEVICE)\n",
    "attention_mask = torch.tensor(sample_encoded['attention_mask']).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "45cf7525-63b2-4106-940d-94dcbfb0f691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0')"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model(input_ids, attention_mask).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "98e598e6-7dd9-421e-9225-5478e049bbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text_2 = ['If the streets are wet, it has rained recently : The streets are wet.']\n",
    "\n",
    "sample_encoded = tokenizer(sample_text_2, truncation=True, padding=True)\n",
    "\n",
    "sample_dataset = SyllogismDataset(sample_encoded, sample_label)\n",
    "trainer.evaluate(sample_dataset)\n",
    "\n",
    "input_ids = torch.tensor(sample_encoded['input_ids']).to(DEVICE)\n",
    "attention_mask = torch.tensor(sample_encoded['attention_mask']).to(DEVICE)\n",
    "\n",
    "torch.argmax(model(input_ids, attention_mask).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4f4ce-88be-4dcc-a1d1-61bd02388a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
